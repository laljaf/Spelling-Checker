{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import heapq #to add more than word to the correction candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "Words=Counter(words(open('typo-0.2.txt').read()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the file with the correct spelling words\n",
    "text='voc-1bwc.txt'\n",
    "\n",
    "#declaring a dictionary with the correct words and their frequency as key\n",
    "word_frequency={}\n",
    "\n",
    "with open(text,'r') as file:\n",
    "    for line in file:\n",
    "        parts=line.strip().split()\n",
    "        if len(parts)==2:\n",
    "            word=parts[1]\n",
    "            frequency=int(parts[0])\n",
    "            word_frequency[word]=frequency\n",
    "\n",
    "#adding the words orig to the dictionary \n",
    "word_frequency['orig']=Words['orig']\n",
    "\n",
    "\n",
    "#adding the words in the file to the dictionary if they are not already in it\n",
    "def WordFrequency(word_to_find):\n",
    "    if word_to_find in word_frequency:\n",
    "        return word_frequency[word_to_find]\n",
    "    else:\n",
    "        return 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-Levenshtein in c:\\users\\jaafar\\appdata\\roaming\\python\\python311\\site-packages (0.23.0)\n",
      "Requirement already satisfied: Levenshtein==0.23.0 in c:\\users\\jaafar\\appdata\\roaming\\python\\python311\\site-packages (from python-Levenshtein) (0.23.0)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.1.0 in c:\\users\\jaafar\\appdata\\roaming\\python\\python311\\site-packages (from Levenshtein==0.23.0->python-Levenshtein) (3.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-Levenshtein\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liste of distances\n",
    "Run only one of the distance sections going  from 1 to 4 then skip to the *Distance Testing* section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Levenshtein Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein #to calculate levenstein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Levensthien distance\n",
    "def distance_levenstein(a, b):\n",
    "    return Levenshtein.distance(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_chosen=distance_levenstein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the closest words (correction) to the word we want to correct using vanilla python Deaumarau levunstein distance\n",
    "def get_closest_word(word, words, n):\n",
    "    closest_words = []\n",
    "    \n",
    "    for candidate in words:\n",
    "        dist = distance_chosen(word, candidate)\n",
    "        if len(closest_words) < n:\n",
    "            closest_words.append((candidate, dist))\n",
    "        else:\n",
    "            # Replace the word with the highest distance if the current distance is smaller\n",
    "            max_dist_word, max_dist = max(closest_words, key=lambda x: x[1])\n",
    "            if dist < max_dist:\n",
    "                closest_words.remove((max_dist_word, max_dist))\n",
    "                closest_words.append((candidate, dist))\n",
    "    \n",
    "    closest_words.sort(key=lambda x: x[1])  # Sort by distance\n",
    "    return [word for word, _ in closest_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demarau Levenshtein Distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance #to calculate the Demarau-Levenstein distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_demarau_levenshtein(a, b):\n",
    "    return textdistance.damerau_levenshtein(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_chosen=distance_demarau_levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaccard Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Word2Vec( vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_distance(a, b):\n",
    "    a = set(a)\n",
    "    b = set(b)\n",
    "    intersection = len(a.intersection(b))\n",
    "    union= len(a.union(b))\n",
    "    return  1 - intersection/union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_chosen=jaccard_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the closest words (correction) to the word we want to correct using heapq\n",
    "def get_closest_word(word, words, n):\n",
    "    closest_words = heapq.nsmallest(n, words, key=lambda x: distance_chosen(word, x))\n",
    "    closest_words.sort(key=lambda tup: tup[1])\n",
    "    return closest_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaro-Winkler distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#library to calculate the Jaro-Winkler distance\n",
    "import jellyfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jaro-Winkler distance\n",
    "def Jaro_Winkler(a, b):\n",
    "    return jellyfish.jaro_winkler(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_chosen=Jaro_Winkler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exception for jaro-winkler distance\n",
    "\n",
    "def get_closest_word(word, words, n):\n",
    "    closest_words = []\n",
    "    \n",
    "    for candidate in words:\n",
    "        dist = distance_chosen(word, candidate)\n",
    "        if len(closest_words) < n:\n",
    "            closest_words.append((candidate, dist))\n",
    "        else:\n",
    "            # Replace the word with the highest distance if the current distance is smaller\n",
    "            max_dist_word, min_dist = min(closest_words, key=lambda x: x[1])\n",
    "            if dist > min_dist:\n",
    "                closest_words.remove((max_dist_word, min_dist))\n",
    "                closest_words.append((candidate, dist))\n",
    "    \n",
    "    # Sort by distance\n",
    "    return [word for word, _ in closest_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing The Distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_typo(match):\n",
    "    orig=match.group(1)\n",
    "    typo=match.group(2)\n",
    "    correction_arr=get_closest_word(typo, word_frequency.keys(),1)\n",
    "    correction = \" \".join(correction_arr)\n",
    "    return f'<correction typo orig=\"{orig}\" >{correction}</correction>'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the name of the distance function to add it to the name of the output file for easier identification\n",
    "dist_string= str(distance_chosen).split()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text=open('typo-0.2.txt','r').read()\n",
    "pattern = r'<typo orig=\"(.*?)\">(.*?)</typo>'\n",
    "modified_text = re.sub(pattern, replace_typo, text)\n",
    "\n",
    "# Write the modified content back to the file\n",
    "with open(f'typocorrected{dist_string}.txt', 'w') as file:\n",
    "    file.write(modified_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jellyfish\n",
    "from nltk.metrics import jaccard_distance\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_closest_word_combined(range, input_word, reference_vocab):\n",
    "# Calculate similarity scores for each metric\n",
    "    scores = []\n",
    "\n",
    "    for word in reference_vocab:\n",
    "        levenshtein_score = 1 - jellyfish.levenshtein_distance(input_word, word) / max(len(input_word), len(word))\n",
    "        jaccard_score = 1 - jaccard_distance(set(input_word), set(word))\n",
    "        jaro_winkler_score = jellyfish.jaro_winkler(input_word, word)\n",
    "        \n",
    "        # You can adjust the weights as needed for each metric\n",
    "        composite_score = 0.4 * levenshtein_score + 0.3 * jaccard_score + 0.3 * jaro_winkler_score\n",
    "        scores.append((word, composite_score))\n",
    "\n",
    "    # Sort candidate words by composite score in descending order\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [word for word, _ in scores[:range]]\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_typo_2(match):\n",
    "    orig=match.group(1)\n",
    "    typo=match.group(2)\n",
    "    correction_arr=get_closest_word_combined(2,typo, word_frequency.keys())\n",
    "    correction = \" \".join(correction_arr)\n",
    "    return f'<correction typo orig=\"{orig}\" >{correction}</correction>'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text=open('typo-0.2.txt','r').read()\n",
    "pattern = r'<typo orig=\"(.*?)\">(.*?)</typo>'\n",
    "modified_text = re.sub(pattern, replace_typo_2, text)\n",
    "\n",
    "# Write the modified content back to the file\n",
    "with open('typocombined.txt', 'w') as file:\n",
    "    file.write(modified_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding distance between the suggested words and the original word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_score_combined(input_word, word):\n",
    "# Calculate similarity scores for each metric\n",
    "    scores = []\n",
    "\n",
    "   \n",
    "    levenshtein_score = 1 - jellyfish.levenshtein_distance(input_word, word) / max(len(input_word), len(word))\n",
    "    jaccard_score = 1 - jaccard_distance(set(input_word), set(word))\n",
    "    jaro_winkler_score = jellyfish.jaro_winkler(input_word, word)\n",
    "       \n",
    "        # You can adjust the weights as needed for each metric\n",
    "    composite_score = 0.4 * levenshtein_score + 0.3 * jaccard_score + 0.3 * jaro_winkler_score\n",
    "    \n",
    "\n",
    "    # Sort candidate words by composite score in descending order\n",
    "    \n",
    "    return composite_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average distance of typocorrectedjaccard_distance is 0.42534662612176893\n",
      "the average distance of typocorrectedJaro_Winkler is 0.5517231534992258\n",
      "the average distance of typocorrected2distance_levenstein is 0.41523627229753357\n"
     ]
    }
   ],
   "source": [
    "texts=['typocorrectedjaccard_distance','typocorrectedJaro_Winkler','typocorrected2distance_levenstein']\n",
    "\n",
    "for textco in texts:\n",
    "    text=open(f'{textco}.txt','r').read()\n",
    "    new_pattern = r'<correction typo orig=\"(.*?)\" >(.*?)</correction>'\n",
    "    total_distances_in_text=[]\n",
    "    #finding the distance between the original word and the corrected word\n",
    "    def find_distance(match):\n",
    "        orig=match.group(1)\n",
    "        correction=match.group(2)\n",
    "        correction_arr=correction.split()\n",
    "        distances=[]\n",
    "        for suggestion in correction_arr:\n",
    "            distances.append(get_closest_score_combined(suggestion,orig))\n",
    "\n",
    "        best_distance=min(distances)\n",
    "        return best_distance\n",
    "\n",
    "    #iterate through all the patterns in the text file\n",
    "    for pattern in re.finditer(new_pattern, text):\n",
    "        total_distances_in_text.append(find_distance(pattern))\n",
    "\n",
    "    #calculating the average distance\n",
    "    average_distance=sum(total_distances_in_text)/len(total_distances_in_text)\n",
    "    print(f'the average distance of {textco} is {average_distance}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
